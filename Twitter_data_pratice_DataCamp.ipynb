{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to End project on Twitter LiveStreaming, Numpy, Pandas DataFrame, NLP, Sentiment Analysis, Network Analysis,\n",
    "Geographical locatio, creating twitter map\n",
    "\n",
    "Why Analyze twitter data?\n",
    "\n",
    "There are millions of tweets created everyday from across the entire world, in many different languages.\n",
    "\n",
    "In this this course we are going to:\n",
    "1.\tCollect twitter data\n",
    "2.\tHow to process text\n",
    "3.\tHow to analyze twitter networks and,\n",
    "4.\tHow to map twitter data geographically \n",
    "\n",
    "Twitter is one of the largest social networking sites in operation right now. While Twitter is far from a comprehensive record of the public conversation, it can help to provide insight into popular trends and important cultural and political moments. If you are a data scientist in industry, analyzing Twitter can be used for task such as:\n",
    "\n",
    "1.\tMarketing or product analysis \n",
    "2.\tMeasure of public opinion on important political or social topics\n",
    "\n",
    "Twitter data has been used to analyze political polarization, public opinion of world leader, and the spread of protest movements. Of course, you can’t access all of what happens on Twitter. It may seem obvious to say, but you can only collect information on what people say, not who is watching passively. \n",
    "\n",
    "If you’re looking to use free tools for doing your analysis, you are also limited in two important ways: \n",
    "1.\tYou cannot collect data from past. If you wanted data on your company from one year ago and you didn’t collect, it at the time you are out of luck.\n",
    "2.\tTwitter only offers a sample of their data for free, what they say is a 1 % sample. However, a 1 % sample of Twitter is still on the order of a few million tweets a day.\n",
    "\n",
    "In each of those tweets, you get a lot of information: the text of the tweet, user profile – like the number of followers and followees the user has – geolocation and other extras. You can also get all of those information from retweets and quoted tweets.\n",
    "\n",
    "Use Case 1:\n",
    "\n",
    "You have been asked to identify the success (or failure) of a particular product. You can use Twitter analysis strategy such as following to  to best executed this:\n",
    "\n",
    "1.\tCollect mentions of the product and identify if people are talking about it positively.\n",
    "2.\tExamine the size of the retweet network mentioning the product.\n",
    "3.\tAnalyzing the geographical penetration of user mentioning the product.\n",
    "\n",
    "Collecting data through the Twitter API\n",
    "\n",
    "By now you might have a good idea of what kind of insights can be gained through Twitter data, let’s look at how to collect some Twitter data ourselves through the Twitter API.\n",
    "\n",
    "API : Application Programming interface\n",
    "\tIt is a method of accessing data from business or government organization.\n",
    "\n",
    "Many, if not most social media companies have some kind of API which are made available to third-party developers and researchers. Twitter has multiple APIs which can be used for different purposes. These change from timt-to-time and Twitter sometimes adds or removes APIs.\n",
    "\n",
    "This includes:\n",
    "1.\tSearch API: Which allows access to tweets from the past week,\n",
    "2.\tAds API: Which focuses on Twitter ads\n",
    "3.\tStraming API: The Streaming API allows us to collect a sample of tweets in real-time based on keywords, user IDs, and locations. The connections stays open until you close it.\n",
    "\n",
    "\n",
    "In this section we will focus on the streaming API.\n",
    "\n",
    "The Steaming API has two endpoints, filter and sample.\n",
    "1.\tFilter endpoint: With the filter endpoint, you can request data on a few hundred keywords, a few thousand usernames, and 25 location ranges.\n",
    "2.\tSample endpoint: With the sample endpoint, Twitter will return a 1 % Random sample of all of the twitter.\n",
    "\n",
    "To collect data from the Streaming API, we are going to use package called ‘tweepy’. It abstracts away much of the work we need to set up a stable Twitter Streaming API connection. When you do this in practice, you’re going to have to set up your own Twitter account and API keys for authentication.\n",
    "\n",
    "from tweepy.streaming import StreamListener import time \n",
    "class SListener(StreamListener): \n",
    "def __init__(self, api = None): \n",
    "self.output = open('tweets_%s.json' % time.strftime('%Y%m%d-%H%M%S'), 'w') \n",
    "self.api = api or API() \n",
    "\n",
    "Our ‘SListener’ object inherits from a general ‘StreamListener’ class included with ‘tweepy’. It opens a new timestamped file in which to store tweets and takes an optional API argument.\n",
    "\n",
    "We first must authenticate with Twitter.\n",
    "\n",
    "tweepy aunthentication\n",
    "\n",
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "auth = OAuthHandler (consumer_key, consumer_secrete)\n",
    "\n",
    "auth.set_access_token ( access_token, access_token_secret)\n",
    "\n",
    "api = API(auth)\n",
    "\n",
    "from tweepy import Stream\n",
    "\n",
    "listen = SListener(api)\n",
    "\n",
    "stream = Steam(auth, listen)\n",
    "\n",
    "stream.sample()\n",
    "\n",
    "OAuthentication, the authentication protocol which the Twitter API uses, requires four tokens which we obtain from the Twitter developer site: the consumer key and consumer secret, and the access token and access token secret.\n",
    "\n",
    "Then we set the access token and the access token secret\n",
    "\n",
    "Finally, we pass the auth object to the tweepy API object.\n",
    "\n",
    "Now we can get the collecting data.\n",
    "\n",
    "If we were going to take a random sample of all of Twitter, we would use the ‘sample’ endpoint\n",
    "\n",
    "Lastly, we use sample collection method to collect data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up tweepy authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = 'Your customer_key'\n",
    "consumer_secret = 'Your_Customer_Secret'\n",
    "access_token = 'Your_access_token'\n",
    "access_token_secret ='Your_token_secret'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import OAuthHandler\n",
    "from tweepy import API\n",
    "\n",
    "# Consumer key authentication\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "# Access key authentication\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# Set up the API with the authentication handler\n",
    "api = API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting data on keywords\n",
    "\n",
    "Now that we've set up the authentication, we can begin to collect Twitter data. \n",
    "Recall that with the Streaming API, we will be collecting real-time Twitter data \n",
    "based on either a sample or filtered by a keyword.\n",
    "\n",
    "In our example, we will collect data on any tweet mentioning #rstats or #python in \n",
    "the tweet text, username, or user description with the filter endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListener(tweepy.StreamListener):\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('tweet_stream.json', 'a') as file:\n",
    "                file.write(data)\n",
    "                print(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: {}\".format(str(e)))\n",
    "        return True\n",
    "\n",
    "\n",
    "twitter_stream = tweepy.Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['suman ale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy.streaming import StreamListener\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "\n",
    "class SListener(StreamListener):\n",
    "    def __init__(self, api = None, fprefix = 'streamer'):\n",
    "        self.api = api or API()\n",
    "        self.counter = 0\n",
    "        self.fprefix = fprefix\n",
    "        self.output  = open('%s_%s.json' % (self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "\n",
    "\n",
    "    def on_data(self, data):\n",
    "        if  'in_reply_to_status' in data:\n",
    "            self.on_status(data)\n",
    "        elif 'delete' in data:\n",
    "            delete = json.loads(data)['delete']['status']\n",
    "            if self.on_delete(delete['id'], delete['user_id']) is False:\n",
    "                return False\n",
    "        elif 'limit' in data:\n",
    "            if self.on_limit(json.loads(data)['limit']['track']) is False:\n",
    "                return False\n",
    "        elif 'warning' in data:\n",
    "            warning = json.loads(data)['warnings']\n",
    "            print(\"WARNING: %s\" % warning['message'])\n",
    "            return\n",
    "\n",
    "\n",
    "    def on_status(self, status):\n",
    "        self.output.write(status)\n",
    "        self.counter += 1\n",
    "        if self.counter >= 20000:\n",
    "            self.output.close()\n",
    "            self.output  = open('%s_%s.json' % (self.fprefix, time.strftime('%Y%m%d-%H%M%S')), 'w')\n",
    "            self.counter = 0\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_delete(self, status_id, user_id):\n",
    "        print(\"Delete notice\")\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_limit(self, track):\n",
    "        print(\"WARNING: Limitation notice received, tweets missed: %d\" % track)\n",
    "        return\n",
    "\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('Encountered error with status code:', status_code)\n",
    "        return \n",
    "\n",
    "\n",
    "    def on_timeout(self):\n",
    "        print(\"Timeout, sleeping for 60 seconds...\")\n",
    "        time.sleep(60)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "\n",
    "# Set up words to track\n",
    "keywords_to_track = ['#rstats', '#python']\n",
    "\n",
    "# Instantiate the SListener object \n",
    "listen = SListener(api)\n",
    "\n",
    "# Instantiate the Stream object\n",
    "stream = Stream(auth, listen)\n",
    "\n",
    "# Begin collecting data\n",
    "stream.filter(track = keywords_to_track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Twitter JSON\n",
    "\n",
    "Now that you’ve collected some Twitter data, lets dig into the structure of the data. The data is returned in Javascrpt Objhect Notation or JSON. JSON is a special data format which is both human-readable and is easily transferred between machines. JSON is structured a lot like Python Objects and is composed of a combination of dictionaries and lists. \n",
    "\n",
    "Twitter JSON contains several important child JSON objects. These are like dictionaries stored in other dictionaries. The important one which we cover in this file are ‘user’, ‘place’, and ‘extended_tweet’.\n",
    "\n",
    "‘user’ contains all the useful information you would want to know about the user who tweeted including their name, their Twitter handle, their Twitter bio, their location, and if they’re verified.\n",
    "\n",
    "‘place’ contains geolocation.\n",
    "\n",
    "‘extended_tweet’ contains over 140 characters.\n",
    "\n",
    "When a tweet is a retweet or contain a quoted tweet, the whole of that tweet will be contained with the Twitter JSON. For retweets, that tweet will be stored in ‘retweeted_status’, and for quoted tweet in ‘quoted_status’.\n",
    "\n",
    "Loading and Accessing JSON tweets\n",
    "\n",
    "import json\n",
    "\n",
    "tweet_json = open(‘tweet-example.json’, ‘r’).read()\n",
    "\n",
    "tweet = json.loads(tweet_json)\n",
    "\n",
    "tweet[‘text’]\n",
    "\n",
    "\n",
    "Then, we’ll used the json package and the loads methods to convert the JSON into a Python dictionary. Lastly, we access the value of interest by using its appropriate key.\n",
    "\n",
    "Child Twitter JSON can be accessed as nested dictionaries.\n",
    "\n",
    "tweet[‘user’][‘screen_name’]\n",
    "\n",
    "tweet[‘user’][‘name’]\n",
    "\n",
    "tweet[‘user’][‘created_at’]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "import json\n",
    "\n",
    "# Convert from JSON to Python object\n",
    "tweet = json.loads(tweet_json)\n",
    "\n",
    "# Print tweet text\n",
    "print(tweet['text'])\n",
    "\n",
    "# Print tweet id\n",
    "print(tweet['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing user data\n",
    "\n",
    "Much of the data which we want to know about the Twitter data is stored in child JSON objects. \n",
    "We will access several parts of the user's information with the user child JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print user handle\n",
    "print(tweet['user']['screen_name'])\n",
    "\n",
    "# Print user follower count\n",
    "print(tweet['user']['followers_count'])\n",
    "\n",
    "# Print user location\n",
    "print(tweet['user']['location'])\n",
    "\n",
    "# Print user description\n",
    "print(tweet['user']['description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing retweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the text of the tweet\n",
    "print(rt['text'])\n",
    "\n",
    "# Print the text of tweet which has been retweeted\n",
    "print(rt['retweeted_status']['text'])\n",
    "\n",
    "# Print the user handle of the tweet\n",
    "print(rt['user']['screen_name'])\n",
    "\n",
    "# Print the user handle of the tweet which has been retweeted\n",
    "print(rt['retweeted_status']['user']['screen_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Twitter text\n",
    "\n",
    "Arguably, the most important part of the tweet is the text. Recall the primary location of the text in the Twitter JSON can be accessed via the ‘text’ key. However, that’s not the only place where we can extract meaningful text-based elements. Twitter has made it possible to use more than 140 characters. For tweets which are over 140 characters, we can access the text through the ‘extended_tweet’ child JSON object and the ‘full_text’ field.\n",
    "\n",
    "Also recall, to get those objects in retweets and quoted tweets, we have to access the ‘retweeted_status’ or ‘quoted_status’ child JSON objects. Once we parse the JSON, we can access the elements by chaining together dictionaries.\n",
    "\n",
    "Flattening Twitter JSON\n",
    "\n",
    "Extended_tweet[‘extended_tweet-full_text’] = extended_tweet[‘extended_tweet’][‘full_text’]\n",
    "\n",
    "To analyze tweets at scale, it’s helpful to put everything into a pandas DataFrame. This allows us to apply certain analysis methods across all rows and multiple columns. However, with the multiple JSON children, we cannot access values in those children easily in columns. To do this, we flatten the JSON by storing the children values in top-level keys. For convenience, we separate the original keys with a dash. \n",
    "\n",
    "To put these all into a DataFrame:\n",
    "1.\twe will have to loop through all of the tweets individually.\n",
    "2.\tWe open a JSON file full of tweets. We’ll then split the JSON by the newline character. \n",
    "3.\tThen for each tweet we parse the JSON with ‘json.loads’, which converts JSON to Python\n",
    "4.\tWe check if the tweet contains a field of interet, such as 280 character tweet in ‘extended_tweet’ if it does, we created a top-level field for it in the tweet dictionary.\n",
    "5.\tThen add this tweet object to a list of tweets\n",
    "6.\tThis is now a list of dictionaries which represent tweets.\n",
    "\n",
    "Because this is a list of dictionaries we can pass it as a argument to the pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the tweet text\n",
    "print(quoted_tweet['text'])\n",
    "\n",
    "# Print the quoted tweet text\n",
    "print(quoted_tweet['quoted_status']['text'])\n",
    "\n",
    "# Print the quoted tweet's extended (140+) text\n",
    "print(quoted_tweet['quoted_status']['extended_tweet']['full_text'])\n",
    "\n",
    "# Print the quoted user location\n",
    "print(quoted_tweet['quoted_status']['user']['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the user screen_name in 'user-screen_name'\n",
    "quoted_tweet['user-screen_name'] = quoted_tweet['user']['screen_name']\n",
    "\n",
    "# Store the quoted_status text in 'quoted_status-text'\n",
    "quoted_tweet['quoted_status-text'] = quoted_tweet['quoted_status']['text']\n",
    "\n",
    "# Store the quoted tweet's extended (140+) text in \n",
    "# 'quoted_status-extended_tweet-full_text'\n",
    "quoted_tweet['quoted_status-extended_tweet-full_text'] = quoted_tweet['quoted_status']['extended_tweet']['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tweet flattening function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tweets(tweets_json):\n",
    "    \"\"\" Flattens out tweet dictionaries so relevant JSON\n",
    "        is in a top-level dictionary.\"\"\"\n",
    "    tweets_list = []\n",
    "    \n",
    "    # Iterate through each tweet\n",
    "    for tweet in tweets_json:\n",
    "        tweet_obj = json.loads(tweet)\n",
    "    \n",
    "        # Store the user screen name in 'user-screen_name'\n",
    "        tweet_obj['user-screen_name'] = tweet_obj['user']['screen_name']\n",
    "    \n",
    "        # Check if this is a 140+ character tweet\n",
    "        if 'extended_tweet' in tweet_obj:\n",
    "            # Store the extended tweet text in 'extended_tweet-full_text'\n",
    "            tweet_obj['extended_tweet-full_text'] = tweet_obj['entended_tweet']['full_text']\n",
    "    \n",
    "        if 'retweeted_status' in tweet_obj:\n",
    "            # Store the retweet user screen name in 'retweeted_status-user-screen_name'\n",
    "            tweet_obj['retweeted_status-user-screen_name'] = tweet_obj['retweeted_status']['user']['screen_name']\n",
    "\n",
    "            # Store the retweet text in 'retweeted_status-text'\n",
    "            tweet_obj['retweeted_status-text'] = tweet_obj['retweeted_status']['text']\n",
    "            \n",
    "        tweets_list.append(tweet_obj)\n",
    "    return tweets_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading tweets into a DataFrame\n",
    "\n",
    "Now it's time to import data into a pandas DataFrame so we can analyze tweets at scale.\n",
    "\n",
    "We will work with a dataset of tweets which contain the hashtag '#rstats' or '#python'. \n",
    "This dataset is stored as a list of tweet JSON objects in data_science_json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Flatten the tweets and store in `tweets`\n",
    "tweets = flatten_tweets(data_science_json)\n",
    "\n",
    "# Create a DataFrame from `tweets`\n",
    "ds_tweets = pd.DataFrame(tweets)\n",
    "\n",
    "# Print out the first 5 tweets from this dataset\n",
    "print(ds_tweets['text'].values[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words\n",
    "Now that we’re able to analyze tweets at scale, it’s time to begin processing tweet text. \n",
    "One of the most basic analyses we can perform is counting the number of times a word or a phrase has appeared in text, and how many times it has appeared compared to other keywords in the text. \n",
    "Counting words is the most basic step we can take in automating the analysis of text. \n",
    "It allows us to convert words into numbers. \n",
    "In practice, counting words can tell us how many times a company, product, or hashtag is mentioned.\n",
    "\n",
    "Counting with str.contains\n",
    "To count the frequency of a particular keywords we’ll use ‘str.contains’ . \n",
    "This is a string method for the pandas Series object which tells us whether or not a row contains the keyword in question. \n",
    "‘str.contains’ return Boolean values. It takes keyword argument case. Setting case = Fasle will make it case insensitive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the tweets and store them\n",
    "flat_tweets = flatten_tweets(data_science_json)\n",
    "\n",
    "# Convert to DataFrame\n",
    "ds_tweets = pd.DataFrame(flat_tweets)\n",
    "\n",
    "# Find mentions of #python in 'text'\n",
    "python = ds_tweets['text'].str.contains('#python', case = False)\n",
    "\n",
    "# Print proportion of tweets mentioning #python\n",
    "print(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for text in all the wrong places\n",
    "\n",
    "Recall that relevant text may not only be in the main text field of the tweet. \n",
    "It may also be in the extended_tweet, the retweeted_status, or the quoted_status. \n",
    "We need to check all of these fields to make sure we've accounted for all the of the relevant text. \n",
    "We'll do this often so we're going to create a function which does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_word_in_tweet(word, data):\n",
    "    \"\"\"Checks if a word is in a Twitter dataset's text. \n",
    "    Checks text and extended tweet (140+ character tweets) for tweets,\n",
    "    retweets and quoted tweets.\n",
    "    Returns a logical pandas Series.\n",
    "    \"\"\"\n",
    "    contains_column = data['text'].str.contains(word, case = False)\n",
    "    contains_column |= data['extended_tweet-full_text'].str.contains(word, case = False)\n",
    "    contains_column |= data['quoted_status-text'].str.contains(word, case = False) \n",
    "    contains_column |= data['quoted_status-extended_tweet-full_text'].str.contains(word, case = False) \n",
    "    contains_column |= data['retweeted_status-text'].str.contains(word, case = False) \n",
    "    contains_column |= data['retweeted_status-extended_tweet-full_text'].str.contains(word, case = False)\n",
    "    return contains_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing #python to #rstats\n",
    "\n",
    "Now that we have a function to check whether or not the word is in the tweet in multiple places, \n",
    "we can deploy this across multiple words and compare them. \n",
    "Let's return to our example with the data science hashtag dataset. \n",
    "We want to see how many times that #rstats occurs compared to #python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mentions of #python in all text fields\n",
    "python = check_word_in_tweet('#python', ds_tweets)\n",
    "\n",
    "# Find mentions of #rstats in all text fields\n",
    "rstats = check_word_in_tweet('#rstats', ds_tweets)\n",
    "\n",
    "# Print proportion of tweets mentioning #python\n",
    "print(\"Proportion of #python tweets:\", np.sum(python) / ds_tweets.shape[0])\n",
    "\n",
    "# Print proportion of tweets mentioning #rstats\n",
    "print(\"Proportion of #rstats tweets:\", np.sum(rstats) / ds_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating time series data frame\n",
    "\n",
    "Time series data is used when we want to analyze or explore variation over time. \n",
    "This is useful when exploring Twitter text data if we want to track the prevalence of a word or set of words.\n",
    "\n",
    "The first step in doing this is converting the DataFrame into a format which can be \n",
    "handled using pandas time series methods. \n",
    "That can be done by converting the index to a datetime type.\n",
    "\n",
    "Time Series\n",
    "\n",
    "Tweets about companies, product, and political issues vary by the day, hour, minute, and even to the second. We want to be able to capture that variation over time. When data is tagged with a date and time, time, this is known as ‘time series’ or ‘over time’ data.\n",
    "\n",
    "We can convert the Twitter format for date. Panda is smart enough to create date time stored in ‘created_at’ – to a date time type with the ‘to_datetime’ method.\n",
    "\n",
    "tweets[‘created_at] = pd.to_datetime(tweets[‘created_at’])\n",
    "\n",
    "Next, we set the index of the DataFrame to the ‘created_at’ column using the ‘set_index’ method\n",
    "\n",
    "tweets = tweets.set_index(‘created_at’)\n",
    "\n",
    "\n",
    "We first generate a summary statistics over the metric we’re interested in. We can use Series method ‘resample’ for this purpose. ‘resample’ allows us to summarize over a time window of our choice and apply a function to it. We will use ‘resample’ with the ‘mean’ method to generate averages over one-minute windows.\n",
    "\n",
    "Lastly we will plot those keyword means over time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series data is used when we want to analyze or explore variation over time. \n",
    "This is useful when exploring Twitter text data if we want to track the prevalence of a word or set of words.\n",
    "\n",
    "The first step in doing this is converting the DataFrame into a format which can \n",
    "be handled using pandas time series methods. That can be done by converting the index to a datetime type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print created_at to see the original format of datetime in Twitter data\n",
    "print(ds_tweets['created_at'].head(5))\n",
    "\n",
    "# Convert the created_at column to np.datetime object\n",
    "ds_tweets['created_at'] = pd.to_datetime(ds_tweets['created_at'])\n",
    "\n",
    "# Print created_at to see new format\n",
    "print(ds_tweets['created_at'].head(5))\n",
    "\n",
    "# Set the index of ds_tweets to created_at\n",
    "ds_tweets = ds_tweets.set_index('created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating mean frequency\n",
    "We need to produce a metric which can be graphed over time. \n",
    "Our function check_word_in_tweet() returns a boolean Series. Remember that the boolean value True == 1, \n",
    "so we can produce a column for each keyword we're interested in and use it to understand its over time prevalence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python column\n",
    "ds_tweets['python'] = check_word_in_tweet('#python', ds_tweets)\n",
    "\n",
    "# Create an rstats column\n",
    "ds_tweets['rstats'] = check_word_in_tweet('#rstats', ds_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average of python column by day\n",
    "mean_python = ds_tweets['python'].resample('1 d').mean()\n",
    "\n",
    "# Average of rstats column by day\n",
    "mean_rstats = ds_tweets['rstats'].resample('1 d').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting mean frequency\n",
    "\n",
    "Lastly, we'll create a per-day average of the mentions of both hashtags and plot them across time. \n",
    "We'll first create proportions from the two boolean Series by the day, then we'll plot them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot mean python by day(green)/mean rstats by day(blue)\n",
    "plt.plot(mean_python.index.day, mean_python, color = 'green')\n",
    "plt.plot(mean_rstats.index.day, mean_rstats, color = 'blue')\n",
    "\n",
    "# Add labels and show\n",
    "plt.xlabel('Day'); plt.ylabel('Frequency')\n",
    "plt.title('Language mentions over time')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is a type of natural language processing method which determines wheter a word, sentence, paragraph, or document is positive or negative. The idea behind sentiment analysis is that we count the words which are positive or negative as a proportion of the words in the rest of the document. Each document then gets a positive and negative score. Sentiment analysis can be useful in gauging reactions to a company, product, politician, or policy.\n",
    "\n",
    "We use the VADER SentimentIntensityAnalyzer included in the Natural Language Toolkit or ‘nltk’. The VADER toolkit handles short text documents like tweets very well because it measures sentiment not only with particular words, but also for emoji and different type of capitalization.\n",
    "\n",
    "\n",
    "Interpreting sentiment Scores\n",
    "\n",
    "A critical part of any type of natural  language processing involves reading the text and assessing wheter the method makes sense compared to a human reading. If we’re attempting to replicate meaning with computational methods, then we have to make sure that meaning has face validity. \n",
    "\n",
    "Face validity means that the metric matches the concept we’re trying to measure. In this cases we want to be sure the sentiment score matches our idea what it means for a tweet to be positive or negative.\n",
    "\n",
    "Each sentiment score from the VADER analyzer provides foru values: negative, neutral, positive, and compound. Compound is a combination of the positive and the negative. It’s a overall assessment which ranges between negative 1 and positive 1. Below 0 is negative and above 0 is positive.\n",
    "\n",
    "Generating Sentiment Averages\n",
    "\n",
    "We generate sentiment averages over time in the same way we generate average prevalence measure. We will extract only the ‘compound’ filed from the sentiment scores.\n",
    "\n",
    "Next, we separate sentiment for each company with our ‘check_word_in_tweet’ function. Lastly we will generate an average value for our time window of one minute. \n",
    "\n",
    "We can plot sentiment scores in the same way we plotted the prevalence of keywords. We set the x-axis to time, and y-axis to our sentiment score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading VADER\n",
    "\n",
    "Sentiment analysis provides us a small glimpse of the meaning of texts with a rather \n",
    "directly interpretable method. While it has its limitations, it's a good place to begin\n",
    "working with textual data. There's a number of out-of-the-box tools in Python we can use \n",
    "for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Instantiate new SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Generate sentiment scores\n",
    "sentiment_scores = ds_tweets['text'].apply(sid.polarity_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating sentiment scores\n",
    "\n",
    "A rough measure of sentiment towards a particular hashtag is to measure average sentiment\n",
    "for tweets mentioning a particular hashtag. It's also possible that other things are happening \n",
    "in that tweet, so it's important to inspect both text as well as metrics generated by automated text methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the text of a positive tweet\n",
    "print(ds_tweets[sentiment > 0.6]['text'].values[0])\n",
    "\n",
    "# Print out the text of a negative tweet\n",
    "print(ds_tweets[sentiment < -0.6]['text'].values[0])\n",
    "\n",
    "# Generate average sentiment scores for #python\n",
    "sentiment_py = sentiment[ check_word_in_tweet('#python', ds_tweets) ].resample('1 d').mean()\n",
    "\n",
    "# Generate average sentiment scores for #rstats\n",
    "sentiment_r = sentiment[ check_word_in_tweet('#rstats', ds_tweets) ].resample('1 d').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting sentiment scores\n",
    "\n",
    "Lastly, let's plot the sentiment of each hashtag over time. \n",
    "This is largely similar to plotting the prevalence of tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average #python sentiment per day\n",
    "plt.plot(sentiment_py.index.day, sentiment_py, color = 'green')\n",
    "\n",
    "# Plot average #rstats sentiment per day\n",
    "plt.plot(sentiment_r.index.day, sentiment_r, color = 'blue')\n",
    "\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Sentiment')\n",
    "plt.title('Sentiment of data science languages')\n",
    "plt.legend(('#python', '#rstats'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Networks\n",
    "\n",
    "We generate those networks using the ‘networkx’ and graph those networks. \n",
    "\n",
    "A ‘node’ is the actor within a network. Within tweeter the node are twitter user. \n",
    "\n",
    "An ‘edge’ or the ‘tie’ specifies the relationship between nodes.\n",
    "\n",
    "We are going to look at multiple types of relationships. Here we will discuss Directed network, \n",
    "which means that an edge only goes on way – the relationship are not mutual. For example you can retweet someone else but they don’t have to retweet you. The source node is where the relationship begins, where the target node is where the relationship ends.\n",
    "\n",
    "Retweets network: The retweet is a directed edge in which the source node retweets the target node. \n",
    "Retweets often signal agreement but not endorsement.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating retweet network\n",
    "\n",
    "Social media is, by nature, networked data. \n",
    "Twitter networks manifest in multiple ways.\n",
    "One of the most important types of networks that appear in Twitter are retweet networks. \n",
    "We can represent these as directed graphs, with the retweeting user as the source and the \n",
    "retweeted person as the target. With Twitter data in our flattened DataFrame, we can import \n",
    "these into networkx and create a retweet network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import networkx\n",
    "import networkx as nx\n",
    "\n",
    "# Create retweet network from edgelist\n",
    "G_rt = nx.from_pandas_edgelist(\n",
    "    sotu_retweets,\n",
    "    source = 'user-screen_name', \n",
    "    target = 'retweeted_status-user-screen_name',\n",
    "    create_using = nx.DiGraph())\n",
    "    \n",
    "# Print the number of nodes\n",
    "print('Nodes in RT network:', len(G_rt.nodes()))\n",
    "\n",
    "# Print the number of edges\n",
    "print('Edges in RT network:', len(G_rt.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating reply network\n",
    "\n",
    "Reply networks have a markedly different structure to retweet networks. While retweet networks often signal agreement, replies can signal discussion, deliberation, and disagreement. The network properties are the same, however: the network is directed, the source is the replier and the target is the user who is being replied to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Create reply network from edgelist\n",
    "G_reply = nx.from_pandas_edgelist(\n",
    "    sotu_replies,\n",
    "    source = 'user-screen_name',\n",
    "    target = 'in_reply_to_screen_name',\n",
    "    created_using = nx.DIGraph())\n",
    "    \n",
    "# Print the number of nodes\n",
    "print('Nodes in reply network:', len(G_reply.nodes()))\n",
    "\n",
    "# Print the number of edges\n",
    "print('Edges in reply network:', len(G_reply.edges()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing retweet network\n",
    "\n",
    "Visualizing retweets networks is an important exploratory data analysis step because it allows us to visually inspect the structure of the network, understand if there is any user that has disproportionate influence, and if there are different spheres of conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random layout positions\n",
    "pos = nx.random_layout(G_rt)\n",
    "\n",
    "# Create size list\n",
    "sizes = [x[1] for x in G_rt.degree()]\n",
    "\n",
    "# Draw the network\n",
    "nx.draw_networkx(G_rt, pos=pos, \n",
    "    with_labels = False, \n",
    "    node_size = sizes,\n",
    "    width = 0.1, alpha = 0.7,\n",
    "    arrowsize = 2, linewidths = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-degree centrality\n",
    "\n",
    "Centrality is a measure of importance of a node to a network. There are many different types of centrality and each of them has slightly different meaning in Twitter networks. We are first focusing on degree centrality, since its calculation is straightforward and has an intuitive explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate in-degree centrality for retweets \n",
    "rt_centrality = nx.in_degree_centrality(G_rt)\n",
    "\n",
    "# Generate in-degree centrality for replies \n",
    "reply_centrality = nx.in_degree_centrality(G_reply)\n",
    "\n",
    "# Store centralities in DataFrame\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(rt.sort_values('degree_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Betweenness Centrality\n",
    "\n",
    "Betweenness centrality for retweet and reply networks signals users who bridge between different Twitter communities. These communities may be tied together by topic or ideology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate betweenness centrality for retweets \n",
    "rt_centrality = nx.betweenness_centrality(G_rt)\n",
    "\n",
    "# Generate betweenness centrality for replies \n",
    "reply_centrality = nx.betweenness_centrality(G_reply)\n",
    "\n",
    "# Store centralities in data frames\n",
    "rt = pd.DataFrame(list(rt_centrality.items()), columns = column_names)\n",
    "reply = pd.DataFrame(list(reply_centrality.items()), columns = column_names)\n",
    "\n",
    "# Print first five results in descending order of centrality\n",
    "print(rt.sort_values('betweenness_centrality', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratios\n",
    "\n",
    "While not strictly a measure of importance to a network, the idea of being \"ratio'd\" is a network measure which is particular to Twitter and is typically used to judge the unpopularity of a tweet. \"The Ratio,\" as it is called, is calculated by taking the number of replies and dividing it by the number of retweets. For our purposes, it makes conceptual sense to take only the in-degrees of both the retweet and reply networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate in-degrees and store in DataFrame\n",
    "degree_rt = pd.DataFrame(list(G_rt.in_degree()), columns = column_names)\n",
    "degree_reply = pd.DataFrame(list(G_reply.in_degree()), columns = column_names)\n",
    "\n",
    "# Merge the two DataFrames on screen name\n",
    "ratio = degree_rt.merge(degree_reply, on = 'screen_name', suffixes = ('_rt', '_reply'))\n",
    "\n",
    "# Calculate the ratio\n",
    "ratio['ratio'] = ratio['degree_reply'] / ratio['degree_rt']\n",
    "\n",
    "# Exclude any tweets with less than 5 retweets\n",
    "ratio = ratio[ratio['degree_rt'] >= 5]\n",
    "\n",
    "# Print out first five with highest ratio\n",
    "print(ratio.sort_values('ratio', ascending = False).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing user-defined location\n",
    "\n",
    "In the slides, we saw that we could obtain user location via user-generated text, including the tweet itself and the location field in the user's description. These are the two most imprecise methods of obtaining user location, but also possibly more readily available.\n",
    "\n",
    "In this exercise, you're going extract the user-defined location from a single example tweet as well as a large set of tweets. We've added another line to our flatten_tweets() function which will allow you to access user-defined location within the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the location of a single tweet\n",
    "print(tweet_json['user']['location'])\n",
    "\n",
    "# Flatten and load the SOTU tweets into a dataframe\n",
    "tweets_sotu = pd.DataFrame(flatten_tweets(tweets_sotu_json))\n",
    "\n",
    "# Print out top five user-defined locations\n",
    "print(tweets_sotu['user-location'].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing bounding box\n",
    "\n",
    "Most tweets which have coordinate-level geographical information attached to them typically come in the form of a bounding box. Bounding boxes are a set of four longitudinal/latitudinal coordinates which denote a particular area in which the user can be located. \n",
    "The bounding box is located in the place value of the Twitter JSON.\n",
    "\n",
    "Geographical Data in Twitter JSON\n",
    "\n",
    "A common – although imprecise – place where location data can be located is in the Twitter itself. Users mention or allude to a location where they are or have visited.\n",
    "\n",
    "Once we have a location we need to resolve it into a latitude and longitude.\n",
    "\n",
    "Another value in the Twitter JSON which offers location information is the user-defined location field. This is a text field which the user fills out. It can be found in the ‘location’ field in the Twitter user JSON.\n",
    "\n",
    "The next value in which we can find geographical information is ‘place’. It is a child JSON object which contains several pieces of information. The most important of these is the ‘bounding_box’, which contains specific coordinates. The bounding box is a special kind of geographical object which allows Twitter to encode some uncertainty in location. As the box indicates it is a set of four longitude and latitude coordinates which create a box which surrounds the location. This is the most common type of geographical object we will find in Twitter data, so we will spend the most time with this. The ‘place’ object also includes country code, full and short name of the place, and the place type, along with some identification data. The bounding box can range from a city block to a whole state or even country. For simplicity’s sake one way we can deal with handling these data is by translating the bounding box into what’s called a centroid or, the center of the bounding box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBoundingBox(place):\n",
    "    \"\"\" Returns the bounding box coordinates.\"\"\"\n",
    "    return place['bounding_box']['coordinates']\n",
    "\n",
    "# Apply the function which gets bounding box coordinates\n",
    "bounding_boxes = tweets_sotu['place'].apply(getBoundingBox)\n",
    "\n",
    "# Print out the first bounding box coordinates\n",
    "print(bounding_boxes.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating the centroid\n",
    "\n",
    "The bounding box can range from a city block to a whole state or even country. For simplicity's sake, one way we can deal with handling these data is by translating the bounding box into what's called a centroid, or the center of the bounding box. The calculation of the centroid is straight forward -- we calculate the midpoints of the lines created by the latitude and longitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCentroid(place):\n",
    "    \"\"\" Calculates the centroid from a bounding box.\"\"\"\n",
    "    # Obtain the coordinates from the bounding box.\n",
    "    coordinates = place['bounding_box']['coordinates'][0]\n",
    "    \n",
    "    longs = np.unique( [x[0] for x in coordinates] )\n",
    "    lats  = np.unique( [x[1] for x in coordinates] )\n",
    "    \n",
    "    if len(longs) == 1 and len(lats) == 1:\n",
    "        # return a single coordinate\n",
    "        return (longs[0], lats[0])\n",
    "    elif len(longs) == 2 and len(lats) == 2:\n",
    "        # If we have two longs and lats, we have a box.\n",
    "        central_long = np.sum(longs) / 2\n",
    "        central_lat  = np.sum(lats) / 2\n",
    "    else:\n",
    "        raise ValueError(\"Non-rectangular polygon not supported.\")\n",
    "\n",
    "    return (central_long, central_lat)\n",
    "\n",
    "# Calculate the centroids of place     \n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Twitter maps\n",
    "\n",
    "Basemap is a library for plotting two dimensional maps in Python. Basemap is built on top of matplotlib so much of the functionality that you got out of matplotlib applies maps you created with it. Basemap transforms coordinates into map projections, while matplotlib draws contours lines and points.\n",
    "\n",
    "A map projection is the method for which we translate latitudes and longitudes of the Earth – which is spherical – onto a two-dimensional plane. Because we will be dealing with latitudes closer to the equator, we will use the Mercator projection, which is denoted by ‘merc’.  We then provide coordinates for the lower lever and upper right corners of the map.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Basemap\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the US bounding box\n",
    "us_boundingbox = [-125, 22, -64, 50] \n",
    "\n",
    "# Set up the Basemap object\n",
    "m = Basemap(llcrnrlon = us_boundingbox[0],\n",
    "            llcrnrlat = us_boundingbox[1],\n",
    "            urcrnrlon = us_boundingbox[2],\n",
    "            urcrnrlat = us_boundingbox[3],\n",
    "            projection='merc')\n",
    "\n",
    "# Draw continents in white,\n",
    "# coastlines and countries in gray\n",
    "m.fillcontinents(color='white')\n",
    "m.drawcoastlines(color='gray')\n",
    "m.drawcountries(color='gray')\n",
    "\n",
    "# Draw the states and show the plot\n",
    "m.drawstates(color='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting centroid coordinates\n",
    "\n",
    "Because we can't plot whole bounding boxes, we summarize the bounding box location into a single point called a centroid. Plotting these on a Basemap map is straightforward. Once we calculate the centroids, we separate the longitudes and latitudes, then pass to the .scatter() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the centroids for the dataset\n",
    "# and isolate longitudue and latitudes\n",
    "centroids = tweets_sotu['place'].apply(calculateCentroid)\n",
    "lon = [x[0] for x in centroids]\n",
    "lat = [x[1] for x in centroids]\n",
    "\n",
    "# Draw continents, coastlines, countries, and states\n",
    "m.fillcontinents(color='white', zorder = 0)\n",
    "m.drawcoastlines(color='gray')\n",
    "m.drawcountries(color='gray')\n",
    "m.drawstates(color='gray')\n",
    "\n",
    "# Draw the points and show the plot\n",
    "m.scatter(lon, lat, latlon = True, alpha = 0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coloring by sentiment\n",
    "\n",
    "We want to be able to differentiate by place with our Twitter analysis. One distinguishing factor between places is how the State of the Union speech was received. For this purpose, we'll use the sentiment analysis we covered in Chapter 2 to evaluate how the speech was received in different parts of the country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sentiment scores\n",
    "sentiment_scores = tweets_sotu['text'].apply(sid.polarity_scores)\n",
    "\n",
    "# Isolate the compound element\n",
    "sentiment_scores = [x['compound'] for x in sentiment_scores]\n",
    "\n",
    "# Draw the points\n",
    "m.scatter(lon, lat, latlon = True, \n",
    "           c = sentiment_scores,\n",
    "           cmap = 'coolwarm', alpha = 0.7)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
